{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "283abe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook, trange\n",
    "import os\n",
    "from pytorch_transformers import BertConfig\n",
    "# from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import re\n",
    "import math\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca23a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-patio",
   "metadata": {},
   "source": [
    "## BERT - fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589d443c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "           output_hidden_states = True, return_dict=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d11cf3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=model, tokenizer=tokenizer):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(inputs['input_ids'])\n",
    "    hidden_states = outputs['hidden_states']\n",
    "    return outputs['pooler_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8f0b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_embedding('fair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad2893e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train_clean.csv', index_col = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b3d1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e777ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under username ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aww matches this background colour seemingly s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man really not trying edit war just that t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more can make any real suggestions improvement...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are hero any chance you remember what ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  target\n",
       "0  explanation why the edits made under username ...       0\n",
       "1  aww matches this background colour seemingly s...       0\n",
       "2  hey man really not trying edit war just that t...       0\n",
       "3  more can make any real suggestions improvement...       0\n",
       "4  you sir are hero any chance you remember what ...       0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "420bc83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143332\n",
       "1     16220\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## down sample majority group\n",
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7300af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_maj = data[data['target'] == 0].sample(16220)\n",
    "data_mio = data[data['target'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f5032c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = pd.concat([data_maj, data_mio], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "204da691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    16220\n",
       "1    16220\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_out['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a7b485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and return a non-unique list of tokenized words\n",
    "    found in the text. Normalize to lowercase, strip punctuation,\n",
    "    remove stop words, drop words of length < 3, strip digits.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('[' + string.punctuation + '0-9\\\\r\\\\t\\\\n]', ' ', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [w for w in tokens if (len(w) > 3) and (w not in ENGLISH_STOP_WORDS)]  # ignore a, an, to, at, be, ...\n",
    "    return tokens\n",
    "\n",
    "def normalizewords(words):\n",
    "    \"\"\"\n",
    "    Given a list of tokens/words, return a new list of normalize words\n",
    "    \"\"\"\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    normal = []\n",
    "    for word, tag in nltk.pos_tag(words):\n",
    "        wtag = tag[0].lower()\n",
    "        wtag = wtag if wtag in ['a', 'r', 'n', 'v'] else None\n",
    "        lemma = lemmatizer.lemmatize(word, wtag) if wtag else word\n",
    "        normal.append(lemma)\n",
    "    return ' '.join(normal)\n",
    "\n",
    "def pre_proess_text(x):\n",
    "    X = []\n",
    "    for i in range(len(x)):\n",
    "        X.append(normalizewords(tokenize(x[i])))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "2b3c9107",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = pre_proess_text(data_out['comment'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "6b840d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out['clean'] = comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "552fbf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out['length'] = data_out['clean'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "d2e58920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32440, 4)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "a63b80f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_out = data_out[data_out['length'] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "437519f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(data_out['comment'].values, data_out['target'].values,\n",
    "                                                      test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "085c2d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "4950c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [t.split() for t in X_train]\n",
    "X_test = [t.split() for t in X_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "d71d3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for line in content:\n",
    "        words = set(line)\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "    return vocab  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "24ac2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = get_vocab(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "327a3e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55244"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "85db412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768, hidden_dropout_prob=0.25,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "8ff5480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2tks(text, max_seq_length=50, padding_start = False):\n",
    "    tok_text = tokenizer.tokenize(text)\n",
    "    if len(tok_text) > max_seq_length:\n",
    "        tok_text = tok_text[:max_seq_length]\n",
    "    ids_text  = tokenizer.convert_tokens_to_ids(tok_text)\n",
    "    padding = [0] * (max_seq_length - len(ids_text))\n",
    "    if padding_start:\n",
    "        out = padding + ids_text\n",
    "    else:\n",
    "        out = ids_text + padding\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "85de5950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6100,  2098,  4183,  5227,  7592,  2551,  1996,  6904,  2278,\n",
       "        6798,  8670, 27488,  1998,  2070, 15814,  2031,  4081,  2008,\n",
       "        1996, 12388,  3791,  2147,  2017,  2031,  2051,  2202,  2298,\n",
       "        1996,  3720,  4283,  5083,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2tks(X_train[600], padding_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "9c5b13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class toxicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.x = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        x = text2tks(x, padding_start=False)\n",
    "        return x, self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "d9cfbe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = toxicDataset(X_train, y_train)\n",
    "valid_ds = toxicDataset(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "ee9fb5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "f7a494d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "53ec6f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertForSequenceClassification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                               output_hidden_states = True, return_dict=True)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        outputs = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits\n",
    "    \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "bca0436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_func, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0\n",
    "        total = 0\n",
    "        weights = [0.1]\n",
    "        class_weight = torch.FloatTensor(weights).cuda()\n",
    "        for x, y in train_dl:\n",
    "            x = x.cuda()\n",
    "            y = y.unsqueeze(1).float().cuda()\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "#             print(logits.shape, y.shape)\n",
    "            loss = loss_func(logits, y)\n",
    "            loss.backward()\n",
    "#             print(loss.grad, logits.grad)\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        epoch_loss = sum_loss/total\n",
    "        val_loss, accuracy = eval_model(model, loss_func)\n",
    "        print('train loss: {:.3f}, valid loss {:.3f} accuracy {:.3f}'.format(epoch_loss, val_loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "ce559d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, loss_func):\n",
    "    model.eval()\n",
    "    sum_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    weights = [0.1, 0.9]\n",
    "    class_weight = torch.FloatTensor(weights).cuda()\n",
    "    for x, y in valid_dl:\n",
    "        x = x.cuda()\n",
    "        y = y.unsqueeze(1).float().cuda()\n",
    "        y_hat = model(x)\n",
    "#         print(y_hat)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y) \n",
    "        y_pred = y_hat > 0\n",
    "        correct += (y_pred.float() == y).float().sum()\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "        total += y.shape[0]\n",
    "    accuracy = correct/total\n",
    "    epoch_loss = sum_loss/total\n",
    "    return epoch_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "90dcc599",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "f09c9da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "ac5297c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# weights = [0.9]\n",
    "# class_weight = torch.FloatTensor(weights).cuda()\n",
    "loss_func = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "37b5237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "32b1e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrlast = .0001\n",
    "lrmain = .000001\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
    "       \n",
    "   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "1ec64c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.422, valid loss 0.303 accuracy 0.870\n",
      "train loss: 0.265, valid loss 0.238 accuracy 0.902\n",
      "train loss: 0.224, valid loss 0.227 accuracy 0.908\n",
      "train loss: 0.199, valid loss 0.209 accuracy 0.915\n",
      "train loss: 0.185, valid loss 0.211 accuracy 0.916\n",
      "train loss: 0.168, valid loss 0.211 accuracy 0.920\n",
      "train loss: 0.153, valid loss 0.207 accuracy 0.919\n",
      "train loss: 0.140, valid loss 0.218 accuracy 0.918\n"
     ]
    }
   ],
   "source": [
    "train_model(model, loss_func, optimizer, num_epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "cc37874e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrlast = .00001\n",
    "lrmain = .000001\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
    "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
    "       \n",
    "   ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "d62e4529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 0.126, valid loss 0.227 accuracy 0.916\n",
      "train loss: 0.113, valid loss 0.238 accuracy 0.916\n",
      "train loss: 0.101, valid loss 0.264 accuracy 0.910\n"
     ]
    }
   ],
   "source": [
    "train_model(model, loss_func, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae95160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "vital-taylor",
   "metadata": {},
   "source": [
    "## BERT -Use as per-train Embedding and applied on CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "fcea8a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rare_words(word_count, min_df=4):\n",
    "    \"\"\" Deletes rare words from word_count\n",
    "    \n",
    "    Deletes words from word_count if they are not in word_vecs\n",
    "    and don't have at least min_df occurrencies in word_count.\n",
    "    \"\"\"\n",
    "    words_delete = []\n",
    "    for word in word_count:\n",
    "        if word_count[word] < min_df:\n",
    "            words_delete.append(word)\n",
    "    for word in words_delete: \n",
    "        word_count.pop(word)\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "d0e6f953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(get_emb, word_count, emb_size=768):\n",
    "    \"\"\"Creates embedding matrix from word vectors. \"\"\"\n",
    "    word_count = delete_rare_words(word_count, min_df=5)\n",
    "    V = len(word_count.keys()) + 2\n",
    "    vocab2index = {}\n",
    "    W = torch.zeros((V, emb_size))\n",
    "    # adding a vector for padding\n",
    "    W[0] = torch.zeros(emb_size)\n",
    "    # adding a vector for rare words \n",
    "    W[1] = torch.from_numpy(np.random.uniform(-0.25,0.25, emb_size))\n",
    "    vocab2index[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_count:\n",
    "        if word in word_count.keys():\n",
    "            word_vec = get_emb(word)\n",
    "            vocab2index[word] = i\n",
    "            W[i] = word_vec\n",
    "            i += 1\n",
    "        else:\n",
    "            W[i] = torch.from_numpy(np.random.uniform(-0.25,0.25, emb_size))\n",
    "            vocab2index[word] = i\n",
    "            i += 1\n",
    "    return W, vocab2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4628952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "c40c0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weight, vocab2index = create_embedding_matrix(get_embedding, word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "5afbbf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23844a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(s, N=80):\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in s])\n",
    "    l = min(N, len(enc1))\n",
    "    enc[:l] = enc1[:l]\n",
    "    return enc, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c9e61287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class toxicDataset2(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.x = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x[index]\n",
    "        x, s = encode_sentence(x, 60)\n",
    "        return x, self.y[index], s\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "07fff732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds2 = toxicDataset2(X, y_train)\n",
    "valid_ds2 = toxicDataset2(X_test, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "27f4288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "train_dl2 = DataLoader(train_ds2, batch_size=batch_size, shuffle=True)\n",
    "valid_dl2 = DataLoader(valid_ds2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a6b92e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in iter(train_dl2):\n",
    "#     aa = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "b2862ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size=768, glove_weights=None):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embedding.weight.data.copy_(glove_weights)\n",
    "            self.embedding.weight.requires_grad = False ## freeze embeddings\n",
    "        self.bn = nn.BatchNorm1d(768)\n",
    "        self.linear = nn.Linear(emb_size, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, s):\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "#         print('emb')\n",
    "#         x = self.bn(x)\n",
    "#         print('norm')\n",
    "#         print(x)\n",
    "        x = torch.nansum(x, dim=1)/ s\n",
    "        x = self.bn(x)\n",
    "        x = self.linear(x)\n",
    "#         print(x.shape)\n",
    "#         print(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "79cb9ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epocs2(model, optimizer, epochs=10):\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total = 0\n",
    "        count = 0\n",
    "        for x, y, s in train_dl2:\n",
    "            count += 1\n",
    "            x = x.long()\n",
    "            y = y.float().unsqueeze(1)\n",
    "            s = s.float().view(s.shape[0], 1)\n",
    "            out = model(x, s)\n",
    "#             print(x)\n",
    "#             print(count)\n",
    "#             print(out)\n",
    "            loss = F.binary_cross_entropy_with_logits(out, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += x.size(0)*loss.item()\n",
    "            total += x.size(0)\n",
    "#             print(count)\n",
    "            if count > 2:\n",
    "#                 print(out) \n",
    "                break\n",
    "        train_loss = total_loss/total\n",
    "        val_loss, val_accuracy = valid_metrics2(model)\n",
    "        \n",
    "        print(\"train_loss %.3f val_loss %.3f val_accuracy %.3f\" % (\n",
    "            train_loss, val_loss, val_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "9812d63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_metrics2(model):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    correct = 0\n",
    "    for x, y, s in valid_dl2:\n",
    "        x = x.long()  #.cuda()\n",
    "        y = y.float().unsqueeze(1)\n",
    "        s = s.float().view(s.shape[0], 1)\n",
    "        batch = y.shape[0]\n",
    "        out = model(x, s)\n",
    "        loss = F.binary_cross_entropy_with_logits(out, y)\n",
    "        sum_loss += batch*(loss.item())\n",
    "        total += batch\n",
    "        pred = (out > 0).float()\n",
    "        correct += (pred == y).float().sum().item()\n",
    "    val_loss = sum_loss/total\n",
    "    val_acc = correct/total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "e7cbe836",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(len(pretrained_weight), emb_size=768, glove_weights=pretrained_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "54268c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in model.parameters():\n",
    "#     print(i)\n",
    "#     print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "037f45d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.916 val_loss 0.684 val_accuracy 0.524\n",
      "train_loss 0.693 val_loss 0.673 val_accuracy 0.621\n",
      "train_loss 0.600 val_loss 0.721 val_accuracy 0.494\n",
      "train_loss 0.633 val_loss 0.670 val_accuracy 0.499\n",
      "train_loss 0.501 val_loss 0.647 val_accuracy 0.666\n",
      "train_loss 0.506 val_loss 0.642 val_accuracy 0.626\n",
      "train_loss 0.503 val_loss 0.629 val_accuracy 0.668\n",
      "train_loss 0.480 val_loss 0.618 val_accuracy 0.673\n",
      "train_loss 0.439 val_loss 0.596 val_accuracy 0.735\n",
      "train_loss 0.439 val_loss 0.589 val_accuracy 0.702\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "train_epocs2(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "1f86ff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.447 val_loss 0.563 val_accuracy 0.761\n",
      "train_loss 0.445 val_loss 0.550 val_accuracy 0.773\n",
      "train_loss 0.422 val_loss 0.541 val_accuracy 0.763\n",
      "train_loss 0.451 val_loss 0.526 val_accuracy 0.771\n",
      "train_loss 0.411 val_loss 0.509 val_accuracy 0.790\n",
      "train_loss 0.413 val_loss 0.495 val_accuracy 0.800\n",
      "train_loss 0.441 val_loss 0.482 val_accuracy 0.800\n",
      "train_loss 0.417 val_loss 0.472 val_accuracy 0.798\n",
      "train_loss 0.445 val_loss 0.463 val_accuracy 0.802\n",
      "train_loss 0.406 val_loss 0.457 val_accuracy 0.802\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_epocs2(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "2f7c1fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 0.409 val_loss 0.425 val_accuracy 0.819\n",
      "train_loss 0.391 val_loss 0.397 val_accuracy 0.834\n",
      "train_loss 0.367 val_loss 0.384 val_accuracy 0.836\n",
      "train_loss 0.349 val_loss 0.366 val_accuracy 0.846\n",
      "train_loss 0.346 val_loss 0.348 val_accuracy 0.854\n",
      "train_loss 0.325 val_loss 0.339 val_accuracy 0.860\n",
      "train_loss 0.314 val_loss 0.331 val_accuracy 0.862\n",
      "train_loss 0.297 val_loss 0.322 val_accuracy 0.867\n",
      "train_loss 0.300 val_loss 0.318 val_accuracy 0.869\n",
      "train_loss 0.314 val_loss 0.316 val_accuracy 0.869\n"
     ]
    }
   ],
   "source": [
    "model.embedding.weight.requires_grad = True\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_epocs2(model, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508dd9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
